{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from keras_bert import load_vocabulary, load_trained_model_from_checkpoint, Tokenizer, get_checkpoint_paths\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, Lambda, Multiply, Masking, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.data_utils import Sequence\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "from nl2sql.utils import read_data, read_tables, read_line,SQL, MultiSentenceTokenizer, Query, Question, Table\n",
    "from nl2sql.utils.optimizer import RAdam\n",
    "from dbengine import DBEngine\n",
    "\n",
    "test_table_file = './data/val.tables.json'\n",
    "\n",
    "# Download pretrained BERT model from https://github.com/ymcui/Chinese-BERT-wwm\n",
    "bert_model_path = './model'\n",
    "paths = get_checkpoint_paths(bert_model_path)\n",
    "test_tables = read_tables(test_table_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_brackets(s):\n",
    "    '''\n",
    "    Remove brackets [] () from text\n",
    "    '''\n",
    "    return re.sub(r'[\\(\\（].*[\\)\\）]', '', s)\n",
    "\n",
    "class QueryTokenizer(MultiSentenceTokenizer):\n",
    "    \"\"\"\n",
    "    Tokenize query (question + table header) and encode to integer sequence.\n",
    "    Using reserved tokens [unused11] and [unused12] for classification\n",
    "    \"\"\"\n",
    "\n",
    "    col_type_token_dict = {'text': '[unused11]', 'real': '[unused12]'}\n",
    "\n",
    "    def tokenize(self, query: Query, col_orders=None):\n",
    "        \"\"\"\n",
    "        Tokenize quesiton and columns and concatenate.\n",
    "\n",
    "        Parameters:\n",
    "        query (Query): A query object contains question and table\n",
    "        col_orders (list or numpy.array): For re-ordering the header columns\n",
    "\n",
    "        Returns:\n",
    "        token_idss: token ids for bert encoder\n",
    "        segment_ids: segment ids for bert encoder\n",
    "        header_ids: positions of columns\n",
    "        \"\"\"\n",
    "\n",
    "        question_tokens = [self._token_cls] + self._tokenize(query.question.text)\n",
    "        header_tokens = []\n",
    "\n",
    "        if col_orders is None:\n",
    "            col_orders = np.arange(len(query.table.header))\n",
    "\n",
    "        header = [query.table.header[i] for i in col_orders]\n",
    "\n",
    "        for col_name, col_type in header:\n",
    "            col_type_token = self.col_type_token_dict[col_type]\n",
    "            col_name = remove_brackets(col_name)\n",
    "            col_name_tokens = self._tokenize(col_name)\n",
    "            col_tokens = [col_type_token] + col_name_tokens\n",
    "            header_tokens.append(col_tokens)\n",
    "\n",
    "        all_tokens = [question_tokens] + header_tokens\n",
    "        return self._pack(*all_tokens)\n",
    "\n",
    "    def encode(self, query:Query, col_orders=None):\n",
    "        tokens, tokens_lens = self.tokenize(query, col_orders)\n",
    "        token_ids = self._convert_tokens_to_ids(tokens)\n",
    "        segment_ids = [0] * len(token_ids)\n",
    "        header_indices = np.cumsum(tokens_lens)\n",
    "        return token_ids, segment_ids, header_indices[:-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = load_vocabulary(paths.vocab)\n",
    "query_tokenizer = QueryTokenizer(token_dict)\n",
    "\n",
    "class SqlLabelEncoder:\n",
    "    \"\"\"\n",
    "    Convert SQL object into training labels.\n",
    "    \"\"\"\n",
    "    def encode(self, sql: SQL, num_cols):\n",
    "        cond_conn_op_label = sql.cond_conn_op\n",
    "\n",
    "        sel_agg_label = np.ones(num_cols, dtype='int32') * len(SQL.agg_sql_dict)\n",
    "        for col_id, agg_op in zip(sql.sel, sql.agg):\n",
    "            if col_id < num_cols:\n",
    "                sel_agg_label[col_id] = agg_op\n",
    "\n",
    "        cond_op_label = np.ones(num_cols, dtype='int32') * len(SQL.op_sql_dict)\n",
    "        for col_id, cond_op, _ in sql.conds:\n",
    "            if col_id < num_cols:\n",
    "                cond_op_label[col_id] = cond_op\n",
    "\n",
    "        return cond_conn_op_label, sel_agg_label, cond_op_label\n",
    "\n",
    "    def decode(self, cond_conn_op_label, sel_agg_label, cond_op_label):\n",
    "        cond_conn_op = int(cond_conn_op_label)\n",
    "        sel, agg, conds = [], [], []\n",
    "\n",
    "        for col_id, (agg_op, cond_op) in enumerate(zip(sel_agg_label, cond_op_label)):\n",
    "            if agg_op < len(SQL.agg_sql_dict):\n",
    "                sel.append(col_id)\n",
    "                agg.append(int(agg_op))\n",
    "            if cond_op < len(SQL.op_sql_dict):\n",
    "                conds.append([col_id, int(cond_op)])\n",
    "        return {\n",
    "            'sel': sel,\n",
    "            'agg': agg,\n",
    "            'cond_conn_op': cond_conn_op,\n",
    "            'conds': conds\n",
    "        }\n",
    "\n",
    "label_encoder = SqlLabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSequence(Sequence):\n",
    "    \"\"\"\n",
    "    Generate training data in batches\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 tokenizer,\n",
    "                 label_encoder,\n",
    "                 is_train=True,\n",
    "                 max_len=160,\n",
    "                 batch_size=32,\n",
    "                 shuffle=True,\n",
    "                 shuffle_header=True,\n",
    "                 global_indices=None):\n",
    "\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.shuffle = shuffle\n",
    "        self.shuffle_header = shuffle_header\n",
    "        self.is_train = is_train\n",
    "        self.max_len = max_len\n",
    "\n",
    "        if global_indices is None:\n",
    "            self._global_indices = np.arange(len(data))\n",
    "        else:\n",
    "            self._global_indices = global_indices\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self._global_indices)\n",
    "\n",
    "    def _pad_sequences(self, seqs, max_len=None):\n",
    "        padded = pad_sequences(seqs, maxlen=None, padding='post', truncating='post')\n",
    "        if max_len is not None:\n",
    "            padded = padded[:, :max_len]\n",
    "        return padded\n",
    "\n",
    "    def __getitem__(self, batch_id):\n",
    "        batch_data_indices =             self._global_indices[batch_id * self.batch_size: (batch_id + 1) * self.batch_size]\n",
    "        batch_data = [self.data[i] for i in batch_data_indices]\n",
    "\n",
    "        TOKEN_IDS, SEGMENT_IDS = [], []\n",
    "        HEADER_IDS, HEADER_MASK = [], []\n",
    "\n",
    "        COND_CONN_OP = []\n",
    "        SEL_AGG = []\n",
    "        COND_OP = []\n",
    "\n",
    "        for query in batch_data:\n",
    "            question = query.question.text\n",
    "            table = query.table\n",
    "\n",
    "            col_orders = np.arange(len(table.header))\n",
    "            if self.shuffle_header:\n",
    "                np.random.shuffle(col_orders)\n",
    "\n",
    "            token_ids, segment_ids, header_ids = self.tokenizer.encode(query, col_orders)\n",
    "            header_ids = [hid for hid in header_ids if hid < self.max_len]\n",
    "            header_mask = [1] * len(header_ids)\n",
    "            col_orders = col_orders[: len(header_ids)]\n",
    "\n",
    "            TOKEN_IDS.append(token_ids)\n",
    "            SEGMENT_IDS.append(segment_ids)\n",
    "            HEADER_IDS.append(header_ids)\n",
    "            HEADER_MASK.append(header_mask)\n",
    "\n",
    "            if not self.is_train:\n",
    "                continue\n",
    "            sql = query.sql\n",
    "\n",
    "            cond_conn_op, sel_agg, cond_op = self.label_encoder.encode(sql, num_cols=len(table.header))\n",
    "\n",
    "            sel_agg = sel_agg[col_orders]\n",
    "            cond_op = cond_op[col_orders]\n",
    "\n",
    "            COND_CONN_OP.append(cond_conn_op)\n",
    "            SEL_AGG.append(sel_agg)\n",
    "            COND_OP.append(cond_op)\n",
    "\n",
    "        TOKEN_IDS = self._pad_sequences(TOKEN_IDS, max_len=self.max_len)\n",
    "        SEGMENT_IDS = self._pad_sequences(SEGMENT_IDS, max_len=self.max_len)\n",
    "        HEADER_IDS = self._pad_sequences(HEADER_IDS)\n",
    "        HEADER_MASK = self._pad_sequences(HEADER_MASK)\n",
    "\n",
    "        inputs = {\n",
    "            'input_token_ids': TOKEN_IDS,\n",
    "            'input_segment_ids': SEGMENT_IDS,\n",
    "            'input_header_ids': HEADER_IDS,\n",
    "            'input_header_mask': HEADER_MASK\n",
    "        }\n",
    "\n",
    "        if self.is_train:\n",
    "            SEL_AGG = self._pad_sequences(SEL_AGG)\n",
    "            SEL_AGG = np.expand_dims(SEL_AGG, axis=-1)\n",
    "            COND_CONN_OP = np.expand_dims(COND_CONN_OP, axis=-1)\n",
    "            COND_OP = self._pad_sequences(COND_OP)\n",
    "            COND_OP = np.expand_dims(COND_OP, axis=-1)\n",
    "\n",
    "            outputs = {\n",
    "                'output_sel_agg': SEL_AGG,\n",
    "                'output_cond_conn_op': COND_CONN_OP,\n",
    "                'output_cond_op': COND_OP\n",
    "            }\n",
    "            return inputs, outputs\n",
    "        else:\n",
    "            return inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / self.batch_size)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self._global_indices)\n",
    "\n",
    "class DataSequence(Sequence):\n",
    "    \"\"\"\n",
    "    Generate training data in batches\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 tokenizer,\n",
    "                 label_encoder,\n",
    "                 is_train=True,\n",
    "                 max_len=160,\n",
    "                 batch_size=32,\n",
    "                 shuffle=True,\n",
    "                 shuffle_header=True,\n",
    "                 global_indices=None):\n",
    "\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.shuffle = shuffle\n",
    "        self.shuffle_header = shuffle_header\n",
    "        self.is_train = is_train\n",
    "        self.max_len = max_len\n",
    "\n",
    "        if global_indices is None:\n",
    "            self._global_indices = np.arange(len(data))\n",
    "        else:\n",
    "            self._global_indices = global_indices\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self._global_indices)\n",
    "\n",
    "    def _pad_sequences(self, seqs, max_len=None):\n",
    "        padded = pad_sequences(seqs, maxlen=None, padding='post', truncating='post')\n",
    "        if max_len is not None:\n",
    "            padded = padded[:, :max_len]\n",
    "        return padded\n",
    "\n",
    "    def __getitem__(self, batch_id):\n",
    "        batch_data_indices =             self._global_indices[batch_id * self.batch_size: (batch_id + 1) * self.batch_size]\n",
    "        batch_data = [self.data[i] for i in batch_data_indices]\n",
    "\n",
    "        TOKEN_IDS, SEGMENT_IDS = [], []\n",
    "        HEADER_IDS, HEADER_MASK = [], []\n",
    "\n",
    "        COND_CONN_OP = []\n",
    "        SEL_AGG = []\n",
    "        COND_OP = []\n",
    "\n",
    "        for query in batch_data:\n",
    "            question = query.question.text\n",
    "            table = query.table\n",
    "\n",
    "            col_orders = np.arange(len(table.header))\n",
    "            if self.shuffle_header:\n",
    "                np.random.shuffle(col_orders)\n",
    "\n",
    "            token_ids, segment_ids, header_ids = self.tokenizer.encode(query, col_orders)\n",
    "            header_ids = [hid for hid in header_ids if hid < self.max_len]\n",
    "            header_mask = [1] * len(header_ids)\n",
    "            col_orders = col_orders[: len(header_ids)]\n",
    "\n",
    "            TOKEN_IDS.append(token_ids)\n",
    "            SEGMENT_IDS.append(segment_ids)\n",
    "            HEADER_IDS.append(header_ids)\n",
    "            HEADER_MASK.append(header_mask)\n",
    "\n",
    "            if not self.is_train:\n",
    "                continue\n",
    "            sql = query.sql\n",
    "\n",
    "            cond_conn_op, sel_agg, cond_op = self.label_encoder.encode(sql, num_cols=len(table.header))\n",
    "\n",
    "            sel_agg = sel_agg[col_orders]\n",
    "            cond_op = cond_op[col_orders]\n",
    "\n",
    "            COND_CONN_OP.append(cond_conn_op)\n",
    "            SEL_AGG.append(sel_agg)\n",
    "            COND_OP.append(cond_op)\n",
    "\n",
    "        TOKEN_IDS = self._pad_sequences(TOKEN_IDS, max_len=self.max_len)\n",
    "        SEGMENT_IDS = self._pad_sequences(SEGMENT_IDS, max_len=self.max_len)\n",
    "        HEADER_IDS = self._pad_sequences(HEADER_IDS)\n",
    "        HEADER_MASK = self._pad_sequences(HEADER_MASK)\n",
    "\n",
    "        inputs = {\n",
    "            'input_token_ids': TOKEN_IDS,\n",
    "            'input_segment_ids': SEGMENT_IDS,\n",
    "            'input_header_ids': HEADER_IDS,\n",
    "            'input_header_mask': HEADER_MASK\n",
    "        }\n",
    "\n",
    "        if self.is_train:\n",
    "            SEL_AGG = self._pad_sequences(SEL_AGG)\n",
    "            SEL_AGG = np.expand_dims(SEL_AGG, axis=-1)\n",
    "            COND_CONN_OP = np.expand_dims(COND_CONN_OP, axis=-1)\n",
    "            COND_OP = self._pad_sequences(COND_OP)\n",
    "            COND_OP = np.expand_dims(COND_OP, axis=-1)\n",
    "\n",
    "            outputs = {\n",
    "                'output_sel_agg': SEL_AGG,\n",
    "                'output_cond_conn_op': COND_CONN_OP,\n",
    "                'output_cond_op': COND_OP\n",
    "            }\n",
    "            return inputs, outputs\n",
    "        else:\n",
    "            return inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / self.batch_size)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self._global_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sel_agg = len(SQL.agg_sql_dict) + 1\n",
    "num_cond_op = len(SQL.op_sql_dict) + 1\n",
    "num_cond_conn_op = len(SQL.conn_sql_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def seq_gather(x):\n",
    "    seq, idxs = x\n",
    "    idxs = K.cast(idxs, 'int32')\n",
    "    return K.tf.batch_gather(seq, idxs)\n",
    "\n",
    "\n",
    "bert_model = load_trained_model_from_checkpoint(paths.config, paths.checkpoint, seq_len=None)\n",
    "for l in bert_model.layers:\n",
    "    l.trainable = True\n",
    "\n",
    "inp_token_ids = Input(shape=(None,), name='input_token_ids', dtype='int32')\n",
    "inp_segment_ids = Input(shape=(None,), name='input_segment_ids', dtype='int32')\n",
    "inp_header_ids = Input(shape=(None,), name='input_header_ids', dtype='int32')\n",
    "inp_header_mask = Input(shape=(None, ), name='input_header_mask')\n",
    "\n",
    "x = bert_model([inp_token_ids, inp_segment_ids]) # (None, seq_len, 768)\n",
    "\n",
    "\n",
    "x_for_cond_conn_op = Lambda(lambda x: x[:, 0])(x) # (None, 768)\n",
    "p_cond_conn_op = Dense(num_cond_conn_op, activation='softmax', name='output_cond_conn_op')(x_for_cond_conn_op)\n",
    "\n",
    "\n",
    "x_for_header = Lambda(seq_gather, name='header_seq_gather')([x, inp_header_ids]) # (None, h_len, 768)\n",
    "header_mask = Lambda(lambda x: K.expand_dims(x, axis=-1))(inp_header_mask) # (None, h_len, 1)\n",
    "\n",
    "x_for_header = Multiply()([x_for_header, header_mask])\n",
    "x_for_header = Masking()(x_for_header)\n",
    "\n",
    "p_sel_agg = Dense(num_sel_agg, activation='softmax', name='output_sel_agg')(x_for_header)\n",
    "\n",
    "x_for_cond_op = Concatenate(axis=-1)([x_for_header, p_sel_agg])\n",
    "p_cond_op = Dense(num_cond_op, activation='softmax', name='output_cond_op')(x_for_cond_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    [inp_token_ids, inp_segment_ids, inp_header_ids, inp_header_mask],\n",
    "    [p_cond_conn_op, p_sel_agg, p_cond_op]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 2 gpus\n"
     ]
    }
   ],
   "source": [
    "NUM_GPUS = 2\n",
    "if NUM_GPUS > 1:\n",
    "    print('using {} gpus'.format(NUM_GPUS))\n",
    "    model = multi_gpu_model(model, gpus=NUM_GPUS)\n",
    "\n",
    "learning_rate = 1e-5\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=RAdam(lr=learning_rate)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputs_to_sqls(preds_cond_conn_op, preds_sel_agg, preds_cond_op, header_lens, label_encoder):\n",
    "    \"\"\"\n",
    "    Generate sqls from model outputs\n",
    "    \"\"\"\n",
    "    preds_cond_conn_op = np.argmax(preds_cond_conn_op, axis=-1)\n",
    "    preds_cond_op = np.argmax(preds_cond_op, axis=-1)\n",
    "\n",
    "    sqls = []\n",
    "\n",
    "    for cond_conn_op, sel_agg, cond_op, header_len in zip(preds_cond_conn_op,\n",
    "                                                          preds_sel_agg,\n",
    "                                                          preds_cond_op,\n",
    "                                                          header_lens):\n",
    "        sel_agg = sel_agg[:header_len]\n",
    "        # force to select at least one column for agg\n",
    "        sel_agg[sel_agg == sel_agg[:, :-1].max()] = 1\n",
    "        sel_agg = np.argmax(sel_agg, axis=-1)\n",
    "\n",
    "        sql = label_encoder.decode(cond_conn_op, sel_agg, cond_op)\n",
    "        sql['conds'] = [cond for cond in sql['conds'] if cond[0] < header_len]\n",
    "\n",
    "        sel = []\n",
    "        agg = []\n",
    "        for col_id, agg_op in zip(sql['sel'], sql['agg']):\n",
    "            if col_id < header_len:\n",
    "                sel.append(col_id)\n",
    "                agg.append(agg_op)\n",
    "\n",
    "        sql['sel'] = sel\n",
    "        sql['agg'] = agg\n",
    "        sqls.append(sql)\n",
    "    return sqls\n",
    "\n",
    "class EvaluateCallback(Callback):\n",
    "    def __init__(self, val_dataseq):\n",
    "        self.val_dataseq = val_dataseq\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pred_sqls = []\n",
    "        for batch_data in self.val_dataseq:\n",
    "            header_lens = np.sum(batch_data['input_header_mask'], axis=-1)\n",
    "            preds_cond_conn_op, preds_sel_agg, preds_cond_op = self.model.predict_on_batch(batch_data)\n",
    "            sqls = outputs_to_sqls(preds_cond_conn_op, preds_sel_agg, preds_cond_op,\n",
    "                                   header_lens, val_dataseq.label_encoder)\n",
    "            pred_sqls += sqls\n",
    "\n",
    "        conn_correct = 0\n",
    "        agg_correct = 0\n",
    "        conds_correct = 0\n",
    "        conds_col_id_correct = 0\n",
    "        all_correct = 0\n",
    "        num_queries = len(self.val_dataseq.data)\n",
    "\n",
    "        true_sqls = [query.sql for query in self.val_dataseq.data]\n",
    "        for pred_sql, true_sql in zip(pred_sqls, true_sqls):\n",
    "            n_correct = 0\n",
    "            if pred_sql['cond_conn_op'] == true_sql.cond_conn_op:\n",
    "                conn_correct += 1\n",
    "                n_correct += 1\n",
    "\n",
    "            pred_aggs = set(zip(pred_sql['sel'], pred_sql['agg']))\n",
    "            true_aggs = set(zip(true_sql.sel, true_sql.agg))\n",
    "            if pred_aggs == true_aggs:\n",
    "                agg_correct += 1\n",
    "                n_correct += 1\n",
    "\n",
    "            pred_conds = set([(cond[0], cond[1]) for cond in pred_sql['conds']])\n",
    "            true_conds = set([(cond[0], cond[1]) for cond in true_sql.conds])\n",
    "\n",
    "            if pred_conds == true_conds:\n",
    "                conds_correct += 1\n",
    "                n_correct += 1\n",
    "\n",
    "            pred_conds_col_ids = set([cond[0] for cond in pred_sql['conds']])\n",
    "            true_conds_col_ids = set([cond[0] for cond in true_sql['conds']])\n",
    "            if pred_conds_col_ids == true_conds_col_ids:\n",
    "                conds_col_id_correct += 1\n",
    "\n",
    "            if n_correct == 3:\n",
    "                all_correct += 1\n",
    "\n",
    "        print('conn_acc: {}'.format(conn_correct / num_queries))\n",
    "        print('agg_acc: {}'.format(agg_correct / num_queries))\n",
    "        print('conds_acc: {}'.format(conds_correct / num_queries))\n",
    "        print('conds_col_id_acc: {}'.format(conds_col_id_correct / num_queries))\n",
    "        print('total_acc: {}'.format(all_correct / num_queries))\n",
    "\n",
    "        logs['val_tot_acc'] = all_correct / num_queries\n",
    "        logs['conn_acc'] = conn_correct / num_queries\n",
    "        logs['conds_acc'] = conds_correct / num_queries\n",
    "        logs['conds_col_id_acc'] = conds_col_id_correct / num_queries\n",
    "\n",
    "model_path = 'task1_best_model.h5'\n",
    "\n",
    "\n",
    "model.load_weights(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import cn2an\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nl2sql.utils import read_data, read_tables, SQL, Query, Question, Table\n",
    "\n",
    "paths = get_checkpoint_paths(bert_model_path)\n",
    "\n",
    "def is_float(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def cn_to_an(string):\n",
    "    try:\n",
    "        return str(cn2an.cn2an(string, 'normal'))\n",
    "    except ValueError:\n",
    "        return string\n",
    "\n",
    "def an_to_cn(string):\n",
    "    try:\n",
    "        return str(cn2an.an2cn(string))\n",
    "    except ValueError:\n",
    "        return string\n",
    "\n",
    "def str_to_num(string):\n",
    "    try:\n",
    "        float_val = float(cn_to_an(string))\n",
    "        if int(float_val) == float_val:\n",
    "            return str(int(float_val))\n",
    "        else:\n",
    "            return str(float_val)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def str_to_year(string):\n",
    "    year = string.replace('年', '')\n",
    "    year = cn_to_an(year)\n",
    "    if is_float(year) and float(year) < 1900:\n",
    "        year = int(year) + 2000\n",
    "        return str(year)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_json(json_file):\n",
    "    result = []\n",
    "    if json_file:\n",
    "        with open(json_file) as file:\n",
    "            for line in file:\n",
    "                result.append(json.loads(line))\n",
    "    return result\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "class QuestionCondPair:\n",
    "    def __init__(self, query_id, question, cond_text, cond_sql, label):\n",
    "        self.query_id = query_id\n",
    "        self.question = question\n",
    "        self.cond_text = cond_text\n",
    "        self.cond_sql = cond_sql\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr_str = ''\n",
    "        repr_str += 'query_id: {}\\n'.format(self.query_id)\n",
    "        repr_str += 'question: {}\\n'.format(self.question)\n",
    "        repr_str += 'cond_text: {}\\n'.format(self.cond_text)\n",
    "        repr_str += 'cond_sql: {}\\n'.format(self.cond_sql)\n",
    "        repr_str += 'label: {}\\n'.format(self.label)\n",
    "        return repr_str\n",
    "\n",
    "\n",
    "class NegativeSampler:\n",
    "    \"\"\"\n",
    "    从 question - cond pairs 中采样\n",
    "    \"\"\"\n",
    "    def __init__(self, neg_sample_ratio=10):\n",
    "        self.neg_sample_ratio = neg_sample_ratio\n",
    "\n",
    "    def sample(self, data):\n",
    "        positive_data = [d for d in data if d.label == 1]\n",
    "        negative_data = [d for d in data if d.label == 0]\n",
    "        negative_sample = random.sample(negative_data,\n",
    "                                        len(positive_data) * self.neg_sample_ratio)\n",
    "        return positive_data + negative_sample\n",
    "\n",
    "\n",
    "class FullSampler:\n",
    "    \"\"\"\n",
    "    不抽样，返回所有的 pairs\n",
    "\n",
    "    \"\"\"\n",
    "    def sample(self, data):\n",
    "        return data\n",
    "\n",
    "class CandidateCondsExtractor:\n",
    "    \"\"\"\n",
    "    params:\n",
    "        - share_candidates: 在同 table 同 column 中共享 real 型 candidates\n",
    "    \"\"\"\n",
    "    CN_NUM = '〇一二三四五六七八九零壹贰叁肆伍陆柒捌玖貮两'\n",
    "    CN_UNIT = '十拾百佰千仟万萬亿億兆点'\n",
    "\n",
    "    def __init__(self, share_candidates=True):\n",
    "        self.share_candidates = share_candidates\n",
    "        self._cached = False\n",
    "\n",
    "    def build_candidate_cache(self, queries):\n",
    "        self.cache = defaultdict(set)\n",
    "        print('building candidate cache')\n",
    "        for query_id, query in tqdm(enumerate(queries), total=len(queries)):\n",
    "            value_in_question = self.extract_values_from_text(query.question.text)\n",
    "\n",
    "            for col_id, (col_name, col_type) in enumerate(query.table.header):\n",
    "                value_in_column = self.extract_values_from_column(query, col_id)\n",
    "                if col_type == 'text':\n",
    "                    cond_values = value_in_column\n",
    "                elif col_type == 'real':\n",
    "                    if len(value_in_column) == 1:\n",
    "                        cond_values = value_in_column + value_in_question\n",
    "                    else:\n",
    "                        cond_values = value_in_question\n",
    "                cache_key = self.get_cache_key(query_id, query, col_id)\n",
    "                self.cache[cache_key].update(cond_values)\n",
    "        self._cached = True\n",
    "\n",
    "    def get_cache_key(self, query_id, query, col_id):\n",
    "        if self.share_candidates:\n",
    "            return (query.table.id, col_id)\n",
    "        else:\n",
    "            return (query_id, query.table.id, col_id)\n",
    "\n",
    "    def extract_year_from_text(self, text):\n",
    "        values = []\n",
    "        num_year_texts = re.findall(r'[0-9][0-9]年', text)\n",
    "        values += ['20{}'.format(text[:-1]) for text in num_year_texts]\n",
    "        cn_year_texts = re.findall(r'[{}][{}]年'.format(self.CN_NUM, self.CN_NUM), text)\n",
    "        cn_year_values = [str_to_year(text) for text in cn_year_texts]\n",
    "        values += [value for value in cn_year_values if value is not None]\n",
    "        return values\n",
    "\n",
    "    def extract_num_from_text(self, text):\n",
    "        values = []\n",
    "        num_values = re.findall(r'[-+]?[0-9]*\\.?[0-9]+', text)\n",
    "        values += num_values\n",
    "\n",
    "        cn_num_unit = self.CN_NUM + self.CN_UNIT\n",
    "        cn_num_texts = re.findall(r'[{}]*\\.?[{}]+'.format(cn_num_unit, cn_num_unit), text)\n",
    "        cn_num_values = [str_to_num(text) for text in cn_num_texts]\n",
    "        values += [value for value in cn_num_values if value is not None]\n",
    "\n",
    "        cn_num_mix = re.findall(r'[0-9]*\\.?[{}]+'.format(self.CN_UNIT), text)\n",
    "        for word in cn_num_mix:\n",
    "            num = re.findall(r'[-+]?[0-9]*\\.?[0-9]+', word)\n",
    "            for n in num:\n",
    "                word = word.replace(n, an_to_cn(n))\n",
    "            str_num = str_to_num(word)\n",
    "            if str_num is not None:\n",
    "                values.append(str_num)\n",
    "        return values\n",
    "\n",
    "    def extract_values_from_text(self, text):\n",
    "        values = []\n",
    "        values += self.extract_year_from_text(text)\n",
    "        values += self.extract_num_from_text(text)\n",
    "        return list(set(values))\n",
    "\n",
    "    def extract_values_from_column(self, query, col_ids):\n",
    "        question = query.question.text\n",
    "        question_chars = set(query.question.text)\n",
    "        unique_col_values = set(query.table.df.iloc[:, col_ids].astype(str))\n",
    "        select_col_values = [v for v in unique_col_values\n",
    "                             if (question_chars & set(v))]\n",
    "        return select_col_values\n",
    "\n",
    "\n",
    "class QuestionCondPairsDataset:\n",
    "    \"\"\"\n",
    "    question - cond pairs 数据集\n",
    "    \"\"\"\n",
    "    OP_PATTERN = {\n",
    "        'real':\n",
    "        [\n",
    "            {'cond_op_idx': 0, 'pattern': '{col_name}大于{value}'},\n",
    "            {'cond_op_idx': 1, 'pattern': '{col_name}小于{value}'},\n",
    "            {'cond_op_idx': 2, 'pattern': '{col_name}是{value}'}\n",
    "        ],\n",
    "        'text':\n",
    "        [\n",
    "            {'cond_op_idx': 2, 'pattern': '{col_name}是{value}'}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    def __init__(self, queries, candidate_extractor, has_label=True, model_1_outputs=None):\n",
    "        self.candidate_extractor = candidate_extractor\n",
    "        self.has_label = has_label\n",
    "        self.model_1_outputs = model_1_outputs\n",
    "        self.data = self.build_dataset(queries)\n",
    "\n",
    "    def build_dataset(self, queries):\n",
    "        if not self.candidate_extractor._cached:\n",
    "            self.candidate_extractor.build_candidate_cache(queries)\n",
    "\n",
    "        pair_data = []\n",
    "        for query_id, query in enumerate(queries):\n",
    "            select_col_id = self.get_select_col_id(query_id, query)\n",
    "            for col_id, (col_name, col_type) in enumerate(query.table.header):\n",
    "                if col_id not in select_col_id:\n",
    "                    continue\n",
    "\n",
    "                cache_key = self.candidate_extractor.get_cache_key(query_id, query, col_id)\n",
    "                values = self.candidate_extractor.cache.get(cache_key, [])\n",
    "                pattern = self.OP_PATTERN.get(col_type, [])\n",
    "                pairs = self.generate_pairs(query_id, query, col_id, col_name,\n",
    "                                               values, pattern)\n",
    "                pair_data += pairs\n",
    "        return pair_data\n",
    "\n",
    "    def get_select_col_id(self, query_id, query):\n",
    "        if self.model_1_outputs:\n",
    "            select_col_id = [cond_col for cond_col, *_ in self.model_1_outputs[query_id]['conds']]\n",
    "        elif self.has_label:\n",
    "            select_col_id = [cond_col for cond_col, *_ in query.sql.conds]\n",
    "        else:\n",
    "            select_col_id = list(range(len(query.table.header)))\n",
    "        return select_col_id\n",
    "\n",
    "    def generate_pairs(self, query_id, query, col_id, col_name, values, op_patterns):\n",
    "        pairs = []\n",
    "        for value in values:\n",
    "            for op_pattern in op_patterns:\n",
    "                cond = op_pattern['pattern'].format(col_name=col_name, value=value)\n",
    "                cond_sql = (col_id, op_pattern['cond_op_idx'], value)\n",
    "                real_sql = {}\n",
    "                if self.has_label:\n",
    "                    real_sql = {tuple(c) for c in query.sql.conds}\n",
    "                label = 1 if cond_sql in real_sql else 0\n",
    "                pair = QuestionCondPair(query_id, query.question.text,\n",
    "                                        cond, cond_sql, label)\n",
    "                pairs.append(pair)\n",
    "        return pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class SimpleTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]')\n",
    "            else:\n",
    "                R.append('[UNK]')\n",
    "        return R\n",
    "\n",
    "\n",
    "def construct_model(paths, use_multi_gpus=True):\n",
    "    token_dict = load_vocabulary(paths.vocab)\n",
    "    tokenizer = SimpleTokenizer(token_dict)\n",
    "\n",
    "    bert_model = load_trained_model_from_checkpoint(\n",
    "        paths.config, paths.checkpoint, seq_len=None)\n",
    "    for l in bert_model.layers:\n",
    "        l.trainable = True\n",
    "\n",
    "    x1_in = Input(shape=(None,), name='input_x1', dtype='int32')\n",
    "    x2_in = Input(shape=(None,), name='input_x2')\n",
    "    x = bert_model([x1_in, x2_in])\n",
    "    x_cls = Lambda(lambda x: x[:, 0])(x)\n",
    "    y_pred = Dense(1, activation='sigmoid', name='output_similarity')(x_cls)\n",
    "\n",
    "    model = Model([x1_in, x2_in], y_pred)\n",
    "    if use_multi_gpus:\n",
    "        print('using multi-gpus')\n",
    "        model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss={'output_similarity': 'binary_crossentropy'},\n",
    "                  optimizer=Adam(1e-5),\n",
    "                  metrics={'output_similarity': 'accuracy'})\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using multi-gpus\n"
     ]
    }
   ],
   "source": [
    "model2, tokenizer = construct_model(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionCondPairsDataseq(Sequence):\n",
    "    def __init__(self, dataset, tokenizer, is_train=True, max_len=120,\n",
    "                 sampler=None, shuffle=False, batch_size=32):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_train = is_train\n",
    "        self.max_len = max_len\n",
    "        self.sampler = sampler\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def _pad_sequences(self, seqs, max_len=None):\n",
    "        return pad_sequences(seqs, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "    def __getitem__(self, batch_id):\n",
    "        batch_data_indices =             self.global_indices[batch_id * self.batch_size: (batch_id + 1) * self.batch_size]\n",
    "        batch_data = [self.data[i] for i in batch_data_indices]\n",
    "\n",
    "        X1, X2 = [], []\n",
    "        Y = []\n",
    "\n",
    "        for data in batch_data:\n",
    "            x1, x2 = self.tokenizer.encode(first=data.question.lower(),\n",
    "                                           second=data.cond_text.lower())\n",
    "            X1.append(x1)\n",
    "            X2.append(x2)\n",
    "            if self.is_train:\n",
    "                Y.append([data.label])\n",
    "\n",
    "        X1 = self._pad_sequences(X1, max_len=self.max_len)\n",
    "        X2 = self._pad_sequences(X2, max_len=self.max_len)\n",
    "        inputs = {'input_x1': X1, 'input_x2': X2}\n",
    "        if self.is_train:\n",
    "            Y = self._pad_sequences(Y, max_len=1)\n",
    "            outputs = {'output_similarity': Y}\n",
    "            return inputs, outputs\n",
    "        else:\n",
    "            return inputs\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.data = self.sampler.sample(self.dataset)\n",
    "        self.global_indices = np.arange(len(self.data))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.global_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / self.batch_size)\n",
    "\n",
    "model2.load_weights('model_best_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_result(qc_pairs, result, threshold):\n",
    "    select_result = defaultdict(set)\n",
    "    for pair, score in zip(qc_pairs, result):\n",
    "        if score > threshold:\n",
    "            select_result[pair.query_id].update([pair.cond_sql])\n",
    "    return dict(select_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building candidate cache\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a6ad32219d423fab7e06ff6780ee92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12/12 [==============================] - 1s 90ms/step\n",
      "SELECT (col_6) FROM Table_69cc8c0c334311e98692542696d6e445 WHERE col_1 == \"长沙\" AND col_2 == \"1.17\"\n",
      "[{'sel': [5], 'agg': [0], 'cond_conn_op': 1, 'conds': [(0, 2, '长沙'), (1, 2, '1.17')]}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#test_json_line = '{\"question\": \"11年第22周的时候啊北京和上海住房成交均价最高的是多少啊\", \"table_id\": \"5a4b46fd312b11e9bef0542696d6e445\"}'\n",
    "\n",
    "while True:\n",
    "    test_json_line=input(\"please input\")\n",
    "    test_data = read_line(test_json_line, test_tables)\n",
    "\n",
    "    test_dataseq = DataSequence(\n",
    "        data=test_data,\n",
    "        tokenizer=query_tokenizer,\n",
    "        label_encoder=label_encoder,\n",
    "        is_train=False,\n",
    "        shuffle_header=False,\n",
    "        max_len=160,\n",
    "        shuffle=False,\n",
    "        batch_size=1\n",
    "    )\n",
    "    pred_sqls = []\n",
    "    header_lens = np.sum(test_dataseq[0]['input_header_mask'], axis=-1)\n",
    "    preds_cond_conn_op, preds_sel_agg, preds_cond_op = model.predict_on_batch(test_dataseq[0])\n",
    "    sql = outputs_to_sqls(preds_cond_conn_op, preds_sel_agg, preds_cond_op,header_lens, test_dataseq.label_encoder)\n",
    "    te_qc_pairs = QuestionCondPairsDataset(test_data,\n",
    "                                       candidate_extractor=CandidateCondsExtractor(share_candidates=True),\n",
    "                                       has_label=False,\n",
    "                                       model_1_outputs=sql)\n",
    "\n",
    "    te_qc_pairs_seq = QuestionCondPairsDataseq(te_qc_pairs, tokenizer,\n",
    "                                           sampler=FullSampler(), shuffle=False, batch_size=1)\n",
    "    te_result = model2.predict_generator(te_qc_pairs_seq, verbose=1)\n",
    "\n",
    "    task2_result = merge_result(te_qc_pairs, te_result, threshold=0.995)\n",
    "    cond = list(task2_result.get(0, []))\n",
    "    sql[0]['conds'] = cond\n",
    "    engine = DBEngine()\n",
    "    print(engine.execute(json.loads(test_json_line)['table_id'], sql[0]['sel'], sql[0]['agg'], sql[0]['conds'], sql[0]['cond_conn_op']))\n",
    "\n",
    "    print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
